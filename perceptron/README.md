# Perceptron Algorithm
헤브의 규칙(Hebb's rule)에 영감을 받아 프랑크 로젠블라트가 제안한 알고리즘이다.
이 알고리즘은 입력 데이터가 선형적으로 분리가 된다면 정답에 수렴됨을 보장한다. (퍼셉트론 수렴 이론)
하지만 입력 데이터가 XOR 문제처럼 선형적으로 분리가 될 수 없다면 학습할 수 없다.

## Algorithm Rule
데이터를 분류하는 결정 경계가 $w_1x_1 + w_2x_2 + b = 0$ 이라는 직선이라고 하자.
가중치 벡터는 $w=(w_1, w_2)$ 이고 입력 데이터의 각 특성은 $x_1, x_2$이다. $b$는 편향(bias)이다.

위 식을 만족하는 점($x_1, x_2$)은 결정 경계 위에 놓여 있는 점이다. $w_1x_1 + w_2x_2 + b > 0$ 이라면 결정 경계 위에 놓여있는 점이다.
$w_1x_1 + w_2x_2 + b < 0$ 라면 결정 경계 아래에 놓여 있는 점이다.

특성 레이블이 양수인 점($p, q$)이 있다. 만약 이 점을 위 직선에 대입했을 때 음수가 나왔다면, 결정 경계는 분류를 잘못하고 있는 것이다. 이런 경우 가중치
$w=(w_1, w_2)$를 업데이트해서 데이터를 올바르게 분류하도록 직선을 조정해야한다.


+ 양수 $(p, q)$에 대해서 음수(-)로 잘못 판단한 경우에는 $w_1, w_2, b$에 각각 $\alpha x_1, \alpha x_2, \alpha$를 더해준다.
+ 음수 $(p, q)$에 대해서 양수(+)로 잘못 판단한 경우에는 $w_1, w_2, b$에 각각 $\alpha x_1, \alpha x_2, \alpha$를 빼준다.

양수 (p, q)에 대해서 음수(-)로 잘못 판단한 경우, 업데이트 규칙은 아래와 같습니다.
+ $w_1' = w_1 + \alpha p$
+ $w_2' = w_2 + \alpha q$
+ $b' = b + \alpha$

단, $\alpha$는 모델이 학습되는 속도를 조정하는 학습률(learning rate). eta라고 읽는다.

그리고 업데이트된 가중치를 각각 $w_1', w_2', b'$이라고 하면 새로운 예측값은 아래와 같습니다.
$$새로운 예측값 = w_1'p + w_2'q + b'$$

여기에 업데이트 규칙을 대입해 봅시다.
$$ (w_1 + \alpha x_1)p + (w_2 + \alpha x_2)q + (b + \alpha) $$

식을 전개하고 재배열하면,

$$ =(w_1p + w_2q + b) + (\alpha p^2 + \alpha q^2 + \alpha) $$
$$ =원래 예측값 + \alpha(p^2 + q^2 + 1) $$


이 식의 의미를 살펴봅시다.

+ 원래 예측값$(w_1p + w_2q + b)$: 위에서 언급 했듯이 음수입니다.
+ $\alpha$(학습률)은 양수입니다.
+ $p^2$과 $q^2$은 제곱했으니 양수입니다.

즉, 새로운 예측값 = (음수) + (양수) 형태가 됩니다. 업데이트 규칙을 적용했더니 원래의 음수값에 양수를 더해주는
결과가 나왔습니다. 이 과정을 반복하면 음수는 결국에는 양수가 될 것 입니다. 이것이 바로 퍼셉트론 모델을 학습하는
규칙입니다.